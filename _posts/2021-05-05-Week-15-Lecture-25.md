---
layout: post
---

Notes and highlights from lecture

## Today's Objectives

* Principal component analysis


## Principal Component Analysis

A problem that arises often in the real world is how to sort through a huge collection of noisy and potentially irrelevant measurements and data to get at the one or two measurements which are the most important.

Take for example the problem of Netflix movie recommendations.
Netflix has a huge amount of information about the watching habits of their users, including when they watch, what they watch, and how long they watch it, not to mention all the ratings that users themselves submit into their own accounts.
Furthermore, each movie itself has a ton more data, such as the genre, the director, the actors/actresses, and the plot.
Out of all of this data, what factors are the most important in determining whether Netflix should recommend a movie to a particular customer?

The frequency and importance of problems like this one have led to many different strategies and algorithms for determining good answers.  One of the most popular methods is Principal Component Analysis (PCA).

In PCA, we start with an $$n\times p$$ matrix of data $$X$$, which can be thought of as experimental data.
The rows of $$X$$ correspond to different experiments and the columns are the various measurements taken for all of the experients.
The PCA algorithm is the as follows.

**PCA Algorithm Steps**

* (1) Create a matrix $$Y$$ whose columns are the columns of $$X$$ minus their mean values.  In MATLAB this can be done using

```Matlab
Y = X - mean(X,1)
```

* (2) Create the **covariance matrix** $$C = Y^TY$$.  In MATLAB this can be done using

```Matlab
C = tranpose(Y)*Y
```

* (3) Calculate the eigenvectors and eigenvalues of the covariance matrix.  In MATLAB this can be done using

```MATLAB
[P,D] = eig(C)
```

* (4) The eigenvector with the largest eigenvalues correspond to the most important measurement, the **principal component** of the system.  Assuming that this eigenvalue is much larger than the other eigenvalues, this means that the most important measurement in the system is the dot product of each row vector of $$Y$$ with the associated eigenvector.
In other words, by knowing that given linear combination of measurements for a particular experiment, we could strongly guess what each of the measurements were individually.

## Mass-Spring System Example

For example imagine that in a certain experiment, we attach various weights to a spring and see how far the spring stretches past its equilibrium length.  Of course, in the actual measurements there are errors in both the measurements of the weights attached and the measurements of how far the spring is stretched.
Below is the data recorded in the lab

|              | weight attached (lbs) | stretch length (ft) |
| ------------ | --------------------- | ------------------- |
| experiment 1 |           1.0         |       0.009         |
| experiment 2 |           1.2         |       0.013         |
| experiment 3 |           1.4         |       0.014         |
| experiment 4 |           1.6         |       0.017         |
| experiment 5 |           1.8         |       0.018         |
| experiment 6 |           2.0         |       0.019         |

Using PCA we can (without any knowledge of physics), determine the most important factors contributing to change in the system.

For the spring experiment above, we calculate

$$C = \left[\begin{array}{cc}
0.70000 & 0.00680\\
0.00680 & 0.00007
\end{array}\right].$$

Using MATLAB, we calculate that the eigenvalues of this matrix are $$0.7000661$$ and $$0.0000039$$, so the principal component of this system corresponds to the larger eigenvalue.  An eigenvector with this eigenvalue is calculated to be $$\binom{0.9999528}{0.00971388}$$.  The first entry corresponds to the width measurement, while the second measurement corresponds to the stretch length.  This means that the most important measurement of the system is the component in the direction of this eigenvector: 

$$0.9999528 w + 0.00971388\ell.$$

where here $$w$$ is the weight added to the spring and $$\ell$$ is the length the spring is stretched.
In other words, if we were somehow able to measure the value of $$0.9999528 w + 0.00971388\ell$$, then we could with great accuracy back out the actual values of $$w$$ and $$\ell$$ for that given experiment.

In fact, each eigenvectors with this maximum eigenvalue is a scalar multiple of this eigenvalue, corresponding geometrically to a straight line.
If we plot our measurements along with the line formed by the eigenvectors corresponding to the principal component, we see that the measurements all lie very close to the line, as shown in the figure below.

:![Spring PCA](/math107spring2021/extras/img/springPCA.jpg)

In actuality, we have rediscovered **Hooke's law** that the stretch length is proportional to the weight added, via some proportionality constant called the **spring constant** which differs from spring to spring, ie.  $$k\ell = w$$.
For our spring, the eigenvector tells us the spring constant should be $$k= 0.00971388/0.9999528 = 0.0097143404$$.





## Reading assignments

* <a target="_parent" href="../../../extras/textbook.pdf">primary text (link)</a>: chapter 12 Section 3
* When Life Is Linear: Ch. 7 (14 pages)

## Additional resources

**Lecture notes:** <a target="_parent" href="https://wcasper.github.io/math107spring2021/extras/notes/2021-05-03-Note-09-50.pdf">notes for lecture (link)</a>

**Lecture video:** <a target="_parent" href="">recording of lecture (link)</a>

The passcode can be found on the <a target="_parent" href="https://csufullerton.instructure.com/courses/3127326/pages/video-lecture-keys">passcode page (link)</a>



